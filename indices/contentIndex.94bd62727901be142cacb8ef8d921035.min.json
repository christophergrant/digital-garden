{"/":{"title":"The Grant Garden ðŸŒ¼","content":"\n## About this blog\n\nThis blog will focus on a few topics, including professional and technical topics like Apache Spark and Linux Foundation Delta Lake, as well as non-professional topics that I am interested in, including art, social theory, and local-first software.\n\nThis blog is living and breathing, and I reserve the right to edit/take down content as I see fit. Don't be surprised if a previous post has been completely and seemingly randomly retooled.\n\n\n## About Christopher Grant\n\nFor work, I am currently employed in the technical pre and post-sales function at Databricks. The title on my theoretical business card is `Senior Specialist Solutions Architect`, which is a mouthful, nonsense title that no-one understands, so I'll give a brief summary of what I do: \n- On good days, I'm [[tuning queries]], helping architect production-ready data pipelines, or contributing to [[Open Source]] software.\n- On bad days, I'm relegated to being a human search engine who dredges up links to publicly available documentation. \n- That being said, the job is fulfilling most of the time and I mostly recommend working at Databricks.\n\nWork admittedly takes up a good amount of my time and energy, but outside of it I enjoy [music](https://www.youtube.com/watch?v=qdmbbiMRe48), [movies](https://www.imdb.com/title/tt0096256/), and social theory.\n\nI currently live in the San Francisco Bay Area.\n\nPlease feel free to reach out! About anything! Online contacts are just as important as those offline.\n\nbackfill(a\u0026t)protonmail.com\n\nhttps://www.linkedin.com/in/nonsense/ - please feel free to send a connection request if I actually know you.\n\n\n## Stack\n\nI use [Obsidian](https://www.google.com/url?sa=t\u0026rct=j\u0026q=\u0026esrc=s\u0026source=web\u0026cd=\u0026cad=rja\u0026uact=8\u0026ved=2ahUKEwiQ5uCcwtn8AhUCNn0KHaJqDlgQFnoECA0QAQ\u0026url=https%3A%2F%2Fobsidian.md%2F\u0026usg=AOvVaw1ILZ6Ax3NYhgLRKojFB5pV) to edit my notes, [Quartz](https://github.com/jackyzha0/quartz) for the website-ized [second brain](https://en.wikipedia.org/wiki/Second_brain), and Github [Repos](https://docs.github.com/en/get-started/quickstart/create-a-repo)/[Pages](https://pages.github.com/)/[Actions](https://github.com/features/actions) for deployment and organization. ","lastmodified":"2023-01-21T23:27:10.278844617Z","tags":null},"/notes/Apache-Spark":{"title":"Apache Spark","content":"\n**Disclaimer** -- I currently work at Databricks, the company that is home to many of the original creators of Apache Spark. Even more than that, I currently work in Sales at Databricks. \n\nThe opinions expressed on this page and on this blog as a whole do not express the opinions of Databricks. And although I'd like to think of myself as neutral, my experience at Databricks has forged my opinions. Interpret this however you'd like.\n\n## Tips for learning Apache Spark in 202x\n\n### Use PySpark\n\nSpark has many dialects including Scala, Python, SQL, R, and other third-party bindings. As it stands right now, Python and SQL are _the_ end-user languages for data. \n\nYou can interoperate between Python and SQL by using PySpark's [spark.sql method](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html), e.g:\n```python\ndf = spark.sql(\"SELECT * FROM table\")\ndf.write.format(\"delta\").save(\"/your/path/here\")\n```\nor even something like:\n```python\nspark.sql(\"\"\"CREATE TABLE table AS \n\t\t  SELECT * FROM other_table\"\"\")\n```\n\nSo you get that base level functionality and familiarity of SQL and the added imperative flexibilty of Python.\n\nI personally skew towards the DataFrame API for data engineering tasks because of the traumatic experiences I have had with massive piles with SQL and YAML. But the point is, you can execute a beautiful hybrid model that is easily taught to those familiar with SQL.\n\nTraditionally, many have questioned whether or not you need to be a Scala guru to use Spark. A spicy take here: there are essentially no reasons to use the Scala dialect nowadays, due to factors like\n- The performance differences between Scala Spark and other Spark dialects has been minimal for a long time. \n- Python is much more accessible to end users than Scala is.\n- Scala's strongly-typed DataSet API never really took off.\n\n### Do not start with RDDs, use DataFrames/SQL instead\n\nRDDs are the foundational and low-level Spark abstraction. DataFrames/SQL are higher level and carry a lot of benefits on top of RDDs. \n\nFor these reasons among others, new Spark applications should use the newer DataFrame/SQL APIs and legacy applications should be rewritten.\n\nThis carries over to other Spark APIs, on the left are the older, lower-level APIs, where the right has the newer APIs:\n- Spark core (RDDs) =\u003e Spark SQL and DataFrames\n- Spark Streaming =\u003e Spark Structured Streaming\n- Spark MLLib =\u003e SparkML\n- GraphX =\u003e GraphFrames\n\n### Use a modern table format\n\nTable formats like Linux Foundation [Delta Lake](https://docs.delta.io/latest/index.html_) and [Apache Iceberg](https://iceberg.apache.org) have changed the data lake game. You get all of the benefits of columnar/binary file formats like Parquet, ORC, Avro, but with the added benefits of:\n- Convinient user APIs for Upserts/Deletes\n- ACID properties\n- Richer schema enforcement capabilities\n- and much more\n\n### Use hydro\n\n\n## Spark Data Sources\n\n**Note: these listings include third-party libraries with varying SDLC standards, use these at your own risk**\n\n**This list is not exhaustive. Please feel free to open a PR against this page to add/remove connectors**\n\n| Connector      | Reference |\n| ----------- | ----------- |\n| CSV/TSV/DSV   |  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html       |\n| JSON   | https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json        |\n| XML   | https://github.com/databricks/spark-xml        |\n| Text   | https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.text.html#pyspark.sql.DataFrameReader.text        |\n| Binary   | https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html        |\n| Avro   | https://spark.apache.org/docs/latest/sql-data-sources-avro.html        |\n| ORC   | https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.orc.html#pyspark.sql.DataFrameReader.orc        |\n| Parquet      | https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html#pyspark.sql.DataFrameReader.parquet       |\n| Kafka   | https://spark.apache.org/docs/3.3.1/structured-streaming-kafka-integration.html#content        |\n| Amazon Kinesis   | https://docs.databricks.com/structured-streaming/kinesis.html        |\n| Azure EventHubs   | https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-kafka-spark-tutorial        |\n| Google Pub/Sub   | Databricks (In preview)        |\n| Delta Lake   | https://delta.io/        |\n| Iceberg   | https://iceberg.apache.org        |\n| Hudi   | https://hudi.apache.org/        |\n| JDBC   | https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.jdbc.html?highlight=jdbc#pyspark.sql.DataFrameReader.jdbc        |\n| MongoDB   | https://www.mongodb.com/docs/spark-connector/current/        |\n| Snowflake   | https://docs.snowflake.com/en/user-guide/spark-connector.html        |\n| Google BigQuery   | https://github.com/GoogleCloudDataproc/spark-bigquery-connector        |\n| Amazon Redshift   | https://docs.databricks.com/external-data/amazon-redshift.html        |\n\n\n## Respected Spark-Related Content Creators\n\n[Holden Karau](http://holdenkarau.com/) - Spark committer, conference talks\n\n[Jacek Laskowski](https://github.com/jaceklaskowski) - GitBooks pages on Apache Spark\n\n[Bartosz Konieczny](https://www.waitingforcode.com/) - WaitingForCode blog\n\n[Nick Karpov](https://www.linkedin.com/in/nick-karpov/) - LinkedIn posts, Delta Users Slack\n\n[Matthew Powers](https://github.com/MrPowers) - Blog posts, various helper libraries\n\n","lastmodified":"2023-01-21T23:27:10.278844617Z","tags":null},"/notes/JSON-schema-inference-with-Apache-Spark":{"title":"JSON schema inferenceÂ  with Apache Spark","content":"\n## post-publishing announcement\n\nplease check out [a library](https://christophergrant.github.io/delta-hydro/api/spark.html#hydro.spark.infer_json_field) I've authored that does this for you. \n\n## title breakdown\n\n- JSON: [JavaScript Object Notation](https://www.json.org/json-en.html), a popular [semi-structured data](https://en.wikipedia.org/wiki/Semi-structured_data) interchange format \n- schema inference: the process of using a computer to derive the emergent structure of semi-structured\n- [Apache Spark](https://spark.apache.org/): a popular [[Open Source]] query execution engine\n\n### a quick note\n**This post is not about inferring the schema of JSON files**, as that issue is already solved with [native DataFrameReader](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html) functionality. \n\n## introduction\n\nIngesting and bringing structure to semi-structured data is an extremely common data-related task. \n\nTraditionally, a large part of developing this structure was to define it manually ([example](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)), which can be extremely tedious and error prone. Luckily, we have tools that will take the data and infer the structure, or schema, from it. Namely, Apache Spark.\n\nInferring the schema of a lot of semi-structured data is one of Apache Spark's (under-appreciated) strengths. But almost all of the examples that showcase this powerful feature focus on file-based sources. What if I want to derive the schema of an in-memory semi-structured payload? What about embedded fields in otherwise structured data? What about ingesting Snowflake VARIANT types? **It turns out, you can do this with a barely-documented feature**. \n\nThe intention of this post is to show how to infer the schema over a lot of JSON payloads. I'm sure that the same methods will work for other semi-structured formats like CSV/TSV/DSV, but these will not be shown in this post. The limiting factor is if the DataFrameReader supports inference across files for that format - [JSON](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala), [CSV](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala) do already(linked only for reference), and I'm sure contributions for any other formats are accepted.\n\nThe structure of this post will be to show one way to apply structure to ingested JSON payloads by using `from_json`. Part of that will be showcasing that, even as of Spark 3.3.0, you need to supply a schema to `from_json`. Then, a couple of methods will be shown for deriving that schema from raw data, which frees you from creating it manually.\n\n\n## from_json\n\nLet's say we have some data with two columns, an integer `id` and JSON string `payload`:\n(don't worry about understanding the details of this code, a vague understanding works)\n```python\ndata = {\"id\": 1, \"payload\": \"\"\"{\"name\": \"christopher\", \"age\": 420}\"\"\"}\ndf = spark.createDataFrame([data])\ndf.show()\n```\n\nResults in:\n```python\n+---+-----------------------------------+\n|id |payload                            |\n+---+-----------------------------------+\n|1  |{\"name\": \"christopher\", \"age\": 420}|\n+---+-----------------------------------+\n```\n\nThis is great, we have our data. But there is one issue:\n```python\n\u003e\u003e\u003e df.schema.simpleString()\n'struct\u003cid:bigint,payload:string\u003e'\n```\n\nPayload is a leaf string field, not a struct. Ideally, name would be selectable. This doesn't work:\n```python\n\u003e\u003e\u003e df.select(\"payload.name\")\n pyspark.sql.utils.AnalysisException: Can't extract value from payload#53: need struct type but got string\n```\n\nTo get to our desired state, we have a [couple of options](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.get_json_object.html#pyspark.sql.functions.get_json_object) , but here's one: `from_json`\n```python\nimport pyspark.sql.functions as F\nschema = 'STRUCT\u003c`age`: BIGINT, `name`: STRING\u003e'\ndf = df.withColumn(\"payload\", F.from_json(\"payload\", schema))\n```\n\n```python\n\u003e\u003e\u003e df.select(\"payload.name\").show()\n+-----------+\n|       name|\n+-----------+\n|christopher|\n+-----------+\n```\n\nNow we're getting somewhere.\n\nBut we're missing one crucial thing: I've [drawn the rest of the owl](https://i.kym-cdn.com/photos/images/original/000/572/078/d6d.jpg). I snuck in `schema`. `from_json`, right now, requires a user-provided schema. \n\nThe rest of this post will show two different ways to have Spark do this for you, and spoiler, one is better than the other. \n\n## alternative 1: schema_of_json\n\n`schema_of_json` is a [native Spark function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_json.html) that takes a JSON string literal and returns its schema.  Here's an example snippet:\n\n```python\nimport pyspark.sql.functions as F\njson_payload = \"\"\"{\"name\": \"christopher\", \"age\":420}\"\"\"\nschema = spark.range(1).select(F.schema_of_json(json_payload).alias(\"schema\")).collect()[0][\"schema\"]\n```\n\nResults in:\n```python\n\u003e\u003e\u003e schema                                                                      \n'STRUCT\u003c`age`: BIGINT, `name`: STRING\u003e'\n```\n\nExcellent. But can we do better? What if I have a lot of rows and their schema is different? Can we derive a super schema that fits it all?\n\n## alternative 2: our inference \"hack\"\n\nIf you take a look at the [documentation for DataFrameReader.json()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json), you'll notice a weird example at the bottom:\n\n```python\n\u003e\u003e\u003e rdd = sc.textFile('python/test_support/sql/people.json')\n\u003e\u003e\u003e df2 = spark.read.json(rdd) \n\u003e\u003e\u003e df2.dtypes \n[('age', 'bigint'), ('name', 'string')]\n```\n\n### explanation\n\nAt line 1, we create an RDD from a text file located at `python/test_support/sql/people.json` using a SparkContext `sc`. The RDD is composed of a single column of type string, one row per line of input.\n\nLine 2 is the motivating part of this post. We create a DataFrame from the RDD by passing it to the SparkSession `spark`'s JSON reader.\n\nLine 4 is the proof that we've inferred the schema, as line 4 is line 3's output and shows multiple columns and differing types (bigint for age), which is desirable when bringing structure to unstructured data.\n\n### application\n\nThis seems promising. But the example doesn't give us what we really want - it's still loading from a file. Let's adapt it:\n\n```python\nimport pyspark.sql.functions as F\njson_payloads = [\"\"\"{\"name\": \"Christopher\"}\"\"\", \"\"\"{\"age\": 420}\"\"\"]\nrdd = sc.parallelize(json_payloads) # needs to be a list of strings\nschema = spark.read.json(rdd).schema\n```\n\nResuts in\n```python\n\u003e\u003e\u003e schema.simpleString()\n'struct\u003cage:bigint,name:string\u003e'\n```\n\nThere we go, same schema as before, and it's the expected one - note that the other schema was uppercased and spaced differently - this doesn't matter. We took two separate lines of JSON, each with different schemas, and created a super schema that accurately describes them both. Although it's a little hacky, alternative 2 is categorically better than alternative 1. It's more robust as it can handle more than one row.\n\n### crossing and dotting\n\nYou can do this with existing DataFrame data\n\n```python\ndata = {\"id\": 1, \"payload\": \"\"\"{\"name\": \"christopher\", \"age\": 420}\"\"\"}\ndf = spark.createDataFrame([data])\npayload_rdd = df.select(\"payload\").rdd.map(lambda row: row[0])\nschema = spark.read.json(payload_rdd).schema\ndf = df.withColumn(\"payload\", F.from_json(\"payload\", schema))\n```\n\nA common application for this would be for exporting data from Snowflake. Snowflake offers a [VARIANT type](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant) that is encoded as a JSON string upon export. Or maybe you have Parquet files with JSON string columns.\n\n## difficulties with semi-structured data\n\nStructure, as it relates to semi-structured data, is best described as a spectrum. There is _well_ structured semi-structured data, and there is _poorly_ structured semi-structured data, and data in-between.\n\nThe above solutions will not work well with _extremely poorly_ structured semi-structured data. What I mean by _extremely poorly_ structured here is if you have, lets say, 5K+ sparsely populated and variable fields. You can give it a try, but there are no guarantees that applying structure to such a monstrosity will work.\n\n## FAQ\n\nQ: What happens when I am missing a field in the schema and apply it to raw data with `from_json`?\n\nA: You will lose that field. If this is something you're afraid of, here's what I suggest: keep the raw text and parsed structure. That way, if you miss a field, you can do a self-referential backfill on your table to correct the parsed structure. Otherwise, if you subscribe to the bronze/silver/gold methodology, and this parsing is not part of your bronze layer (and it shouldn't be), you can simply stream from the layer that comes before and do a backfill that way.\n\n## conclusion\n\nThis post went over a couple of ways of inferring the schema of JSON payloads. You can apply these methods to lessen your development burden and avoid mistakes.\n\nThe desired terminal state of this post is obsolescence. Hopefully we as Spark developers  get native support for semi-structured schema inference soon. Or some kind of JSON shredding functionality.","lastmodified":"2023-01-21T23:27:10.278844617Z","tags":null},"/notes/The-concretized-door":{"title":"The concretized door","content":"\nImagine a doorframe. The doorframe stands on its own, surrounded by nothing. You find it to be beautiful and inviting. You're not really sure what lies behind it, but you want to pass through and experience what's on the other side.\n\nBut once you open that door, you are greeted with a solid wall of cold, hard concrete. \n\nThis is a bit shocking, and your thoughts start racing:\n* Why is this concrete here?\n* Is it my fault that the concrete is here? Could I have done something differently in the past to not have experienced this concrete restriction?\n* And finally, how thick is the concrete? Could I bust through it to experience the other side?\n\n## Historical analysis - why is the concrete here?\n\nThe answer to the first question, if you can even accurately obtain it, is usually a long story - and to be honest, historical stuff like this usually bores me. So Iet's discuss the latter two points.\n\n## Playing the blame game - is this my fault?\n\nYou think to yourself: has the concrete always been here, like this, for me? Could I have conducted myself differently and received a different outcome? \n\nWhat If I spent more time contemplating the beauty of the door? What If I had a slightly gentler grasp when I opened the door? What if I was having a bad hair day that I opened the door: would that have made a difference? \n\nOr more deeply, maybe this door has so much concrete behind it not for reasons that I can control, but something I cannot. Maybe the door is racist. Maybe it's sexist. \n\nIn other words, maybe it's not **what** you've done, but **who** you are. But the lines between what we've done and who we are aren't always clear. Who we are has a strong effect on what we've done.\n\n## Breaking down barriers - can I bust through the concrete?\n\nThe concrete is there. But you very much desire to break through this concrete and get to the other side.\n\nThe problems are:\n* You're unsure how thick the concrete is. It could be entirely possible that it's very thin concrete. But it's also possible that the concrete is infinitely thick, and that no matter how much effort you put in, you're wasting your time.\n* Let's say you do manage to break through the concrete with your impressive willpower. \n\t* Would it be worth it? What if your expectations of the inner-door experience let you down? This question applies even if the concrete isn't there - but in this instance it is. Does working through the concrete devalue the inner-door experience?\n\t* Would you feel dirty for having gone through the entire process? What if you know that others have passed through that same door, but experienced no concrete resistance. \n\n### Closing\n\nA lot of people are scared to open doors because they are afraid that they will be met with concrete.\n\nI'm of the opinion that yes, we can probably minimize the concrete that we face in our lives by changing our behavior, but that people are complicated, and a lot of concrete will be there regardless of what you do. Deservedly or not.\n\nI'll try and continue opening these doors - because although there is a lot of concrete out there, there are a lot of inner-door experiences that make life worth living. To find the ecstatic experiences, you have to face a lot of concrete.","lastmodified":"2023-01-21T23:27:10.278844617Z","tags":null}}