{"/":{"title":"New Beginnings.","content":"\nCreate more. Consume less. \n\n## About this blog\n\nThis blog will focus on a few topics, including professional and technical topics like Apache Spark and Linux Foundation Delta Lake, as well as non-professional topics that I am interested in, including art, social theory, and local-first software.\n\n\n## About Christopher Grant\n\nFor work, I am currently employed in the technical pre and post-sales function at Databricks. The title on my theoretical business card is `Senior Specialist Solutions Architect`, which is a mouthful, nonsense title that no-one understands, so I'll give a brief summary of what I do: \n- On good days, I'm [[tuning queries]], helping architect production-ready data pipelines, or contributing to [[open source]] software.\n- On bad days, I'm relegated to being a human search engine who dredges up links to publicly available documentation. \n- That being said, the job is fulfilling most of the time and I mostly recommend working at Databricks.\n\nWork admittedly takes up a good amount of my time and energy, but outside of it I enjoy music, movies, philosophy, and (mostly competitive) video games.\n\nI currently live in the San Francisco Bay Area.\n\nPlease feel free to reach out! About anything! Online contacts are just as important as those offline.\n\nbackfill(a\u0026t)protonmail.com\n\nhttps://www.linkedin.com/in/nonsense/ - please feel free to send a connection request if I actually know you.\n","lastmodified":"2022-12-24T21:23:31.208957566Z","tags":null},"/notes/Apache-Spark":{"title":"Apache Spark","content":"\n## Respected Spark-Related Content Creators\n\n[Jacek Laskowski](https://github.com/jaceklaskowski) - GitBooks pages on Apache Spark\n\n[Bartosz Konieczny](https://www.waitingforcode.com/) - WaitingForCode blog\n\n[Nick Karpov](https://www.linkedin.com/in/nick-karpov/) - LinkedIn posts, Delta Users Slack\n\n[Matthew Powers](https://github.com/MrPowers) - Blog posts, various helper libraries\n\n","lastmodified":"2022-12-24T21:23:31.208957566Z","tags":null},"/notes/JSON-schema-inference-with-Apache-Spark":{"title":"JSON schema inferenceÂ  with Apache Spark","content":"\n## title breakdown\n\n- JSON: [JavaScript Object Notation](https://www.json.org/json-en.html), a popular [semi-structured data](https://en.wikipedia.org/wiki/Semi-structured_data) interchange format \n- schema inference: the process of using a computer to derive the emergent structure of semi-structured\n- [Apache Spark](https://spark.apache.org/): a popular [[Open Source]] query execution engine\n\n## introduction\n\nIngesting and bringing structure to semi-structured data is an extremely common data-related task. \n\nTraditionally, a large part of developing this structure was to define it manually ([example](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)), which can be extremely tedious and error prone. Luckily, we have tools that will take the data and infer the structure, or schema, from it. Namely, Apache Spark.\n\nInferring the schema of a lot of semi-structured data is one of Apache Spark's (under-appreciated) strengths. But almost all of the examples that showcase this powerful feature focus on file-based sources. What if I want to derive the schema of an in-memory semi-structured payload? What about embedded fields in otherwise structured data? What about ingesting Snowflake VARIANT types? **It turns out, you can do this with a barely-documented feature**. \n\nThe intention of this post is to show how to infer the schema over a lot of JSON payloads. I'm sure that the same methods will work for other semi-structured formats like CSV/TSV/DSV, but these will not be shown in this post. The limiting factor is if the DataFrameReader supports inference across files for that format - [JSON](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala), [CSV](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala) (linked only for reference).\n\nThe structure of this post will be to show one way to apply structure to ingested JSON payloads by using `from_json`. Part of that will be showcasing that, even as of Spark 3.3.0, you need to supply a schema to `from_json`. Then, a couple of methods will be shown for deriving that schema from raw data, which frees you from creating it manually.\n\n\n## from_json\n\nLet's say we have some data with two columns, an integer `id` and JSON string `payload`:\n(don't worry about understanding the details of this code, a vague understanding works)\n```python\ndata = {\"id\": 1, \"payload\": \"\"\"{\"name\": \"christopher\", \"age\": 420}\"\"\"}\ndf = spark.createDataFrame([data])\ndf.show()\n```\n\nResults in:\n```python\n+---+-----------------------------------+\n|id |payload                            |\n+---+-----------------------------------+\n|1  |{\"name\": \"christopher\", \"age\": 420}|\n+---+-----------------------------------+\n```\n\nThis is great, we have our data. But there is one issue:\n```python\n\u003e\u003e\u003e df.schema.simpleString()\n'struct\u003cid:bigint,payload:string\u003e'\n```\n\nPayload is a leaf string field, not a struct. Ideally, name would be selectable. This doesn't work:\n```python\n\u003e\u003e\u003e df.select(\"payload.name\")\n pyspark.sql.utils.AnalysisException: Can't extract value from payload#53: need struct type but got string\n```\n\nTo get to our desired state, we have a [couple of options](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.get_json_object.html#pyspark.sql.functions.get_json_object) , but here's one: `from_json`\n```python\nimport pyspark.sql.functions as F\nschema = 'STRUCT\u003c`age`: BIGINT, `name`: STRING\u003e'\ndf = df.withColumn(\"payload\", F.from_json(\"payload\", schema))\n```\n\n```python\n\u003e\u003e\u003e df.select(\"payload.name\").show()\n+-----------+\n|       name|\n+-----------+\n|christopher|\n+-----------+\n```\n\nNow we're getting somewhere.\n\nBut we're missing one crucial thing, I've [drawn the rest of the owl](https://i.kym-cdn.com/photos/images/original/000/572/078/d6d.jpg). I snuck in `schema`. `from_json`, right now, requires a user-provided schema. \n\nThe rest of this post will show two different ways to have Spark do this for you, and spoiler, one is better than the other. \n\n## alternative 1: schema_of_json\n\n`schema_of_json` is a [native Spark function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.schema_of_json.html) that takes a JSON string literal and returns its schema.  Here's an example snippet:\n\n```python\nimport pyspark.sql.functions as F\njson_payload = \"\"\"{\"name\": \"christopher\", \"age\":420}\"\"\"\nschema = spark.range(1).select(F.schema_of_json(json_payload).alias(\"schema\")).collect()[0][\"schema\"]\n```\n\nResults in:\n\n```python\n\u003e\u003e\u003e schema                                                                      \n'STRUCT\u003c`age`: BIGINT, `name`: STRING\u003e'\n```\n\nExcellent. But can we do better? What if I have a lot of rows and their schema is different? Can we derive a super schema that fits it all?\n\n## alternative 2: our inference \"hack\"\n\nIf you take a look at the [documentation for DataFrameReader.json()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json), you'll notice a weird example at the bottom:\n\n```python\n\u003e\u003e\u003e rdd = sc.textFile('python/test_support/sql/people.json')\n\u003e\u003e\u003e df2 = spark.read.json(rdd) \n\u003e\u003e\u003e df2.dtypes \n[('age', 'bigint'), ('name', 'string')]\n```\n\n#### explanation\n\nAt line 1, we create an RDD from a text file located at `python/test_support/sql/people.json` using a SparkContext `sc`. The RDD is composed of a single column of type string, one row per line of input.\n\nLine 2 is the motivating part of this post. We create a DataFrame from the RDD by passing it to the SparkSession `spark`'s JSON reader.\n\nLine 4 is the proof that we've inferred the schema, as line 4 is line 3's output and shows multiple columns and differing types (bigint for age), which is desirable when bringing structure to unstructured data.\n\n#### application\n\nThis seems promising. But the example doesn't give us what we really want - it's still loading from a file. Let's adapt it:\n\n```python\nimport pyspark.sql.functions as F\njson_payloads = [\"\"\"{\"name\": \"Christopher\"}\"\"\", \"\"\"{\"age\": 420}\"\"\"]\nrdd = sc.parallelize(json_payloads) # needs to be a list of strings\nschema = spark.read.json(rdd).schema\n```\n\nResuts in\n```python\n\u003e\u003e\u003e schema.simpleString()\n'struct\u003cage:bigint,name:string\u003e'\n```\n\nThere we go. We took two separate lines of JSON, each with different schemas, and created a super schema that accurately describes them both. Although it's a little hacky, alternative 2 is categorically better than alternative 1. It's more robust as it can handle more than one row.\n\n#### crossing and dotting\n\nYou can do this with existing DataFrame data\n\n```python\ndata = {\"id\": 1, \"payload\": \"\"\"{\"name\": \"christopher\", \"age\": 420}\"\"\"}\ndf = spark.createDataFrame([data])\npayload_rdd = df.select(\"payload\").rdd.map(lambda row: row[0])\nschema = spark.read.json(payload_rdd).schema\ndf = df.withColumn(\"payload\", F.from_json(\"payload\", schema))\n```\n\nA common application for this would be for exporting data from Snowflake. Snowflake offers a [VARIANT type](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant) that is encoded as a JSON string upon export. Or maybe you have Parquet files with JSON columns.\n\n## difficulties with semi-structured data\n\nStructure, as it relates to semi-structured data, is best described as a spectrum. There is _well_ structured semi-structured data, and there is _poorly_ structured semi-structured data, and data in-between.\n\nThe above solutions will not work well with _extremely poorly_ structured semi-structured data. What I mean by _extremely poorly_ structured here is if you have, lets say, 5K+ sparsely populated and variable fields. You can give it a try, but there are no guarantees that applying structure to such a monstrosity will work.\n\n## FAQ\n\nQ: What happens when I am missing a field in the schema and apply it to raw data with `from_json`?\n\nA: You will lose that field. If this is something you're afraid of, here's what I suggest: keep the raw text and parsed structure. That way, if you miss a field, you can do a self-referential backfill on your table to correct the parsed structure. Otherwise, if you subscribe to the bronze/silver/gold methodology, and this parsing is not part of your bronze layer (and it shouldn't be), you can simply stream from the layer that comes before and do a backfill that way.\n\n## conclusion\n\nThis post went over a couple of ways of inferring the schema of JSON payloads. You can apply these methods to lessen your development burden and avoid mistakes.\n\nThe desired terminal state of this post is obsolescence. Hopefully we as Spark developers  get native support for semi-structured schema inference soon. Or some kind of JSON shredding functionality.","lastmodified":"2022-12-24T21:23:31.208957566Z","tags":null},"/notes/Open-Source":{"title":"","content":"","lastmodified":"2022-12-24T21:23:31.208957566Z","tags":null}}